<!DOCTYPE html>
<html lang="en">
  <!-- Head tag -->
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Title -->
  
  <title>Linear Discriminant Analysis(LDA)简介 - Ghost in the Hoop</title>

  <!--Favicon-->
  <link rel="icon" href="favicon/favicon.ico">

  <!--Description-->
  
      <meta name="description" content="Linear Discriminant AnalysisBy Yang Chen

贝叶斯分类器
引入「LDA」之前，我们先介绍贝叶斯分类器「Bayes Classifier」。它是一个重要的基准，因为在所有分类器中它的测试集误差率「Test Error Rate」是最低的。任何其他分类器都可以">
  

  <!--Author-->
  
      <meta name="author" content="Eric Chen">
  

  <!-- Pure CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
  <link href="https://fonts.googleapis.com/css?family=Crimson+Text|Open+Sans:300,800" rel="stylesheet">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/styles.css">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <!-- Google Analytics -->
  

</head>


  <body>
  	<div class="container-fluid navbar-container m-sm-5">
      <!-- Header -->
      <nav class="navbar navbar-toggleable-sm navbar-light px-1 py-3 my-3 mb-sm-5">
  <a class="navbar-brand ml-2" href="/">Ghost in the Hoop</a>
  <button class="navbar-toggler navbar-toggler-right py-2" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse text-center" id="navbarCollapse">
    <ul class="navbar-nav ml-auto my-auto">
      
    </ul>
    <hr class="hidden-md-up" />
  </div>
</nav>


  		<div class="row">
  			<div class="col-12 mb-4">
  <img class="img-fluid project-img" src="/images/unsplash.jpg" alt="Linear Discriminant Analysis(LDA)简介">
</div>
<div class="col-lg-4 col-12 pt-3 px-4 pr-lg-5">
  <h1>Linear Discriminant Analysis(LDA)简介</h1>
</div>
<div class="col-lg-8 col-12 pt-lg-3 mb-4 pl-lg-5 px-lg-0 px-4 portfolio-content">
  <h1 id="Linear-Discriminant-Analysis"><a href="#Linear-Discriminant-Analysis" class="headerlink" title="Linear Discriminant Analysis"></a>Linear Discriminant Analysis</h1><p>By Yang Chen</p>
<ul>
<li><p><strong>贝叶斯分类器</strong></p>
<p>引入「LDA」之前，我们先介绍贝叶斯分类器「Bayes Classifier」。它是一个重要的基准，因为在所有分类器中它的测试集误差率「Test Error Rate」是最低的。任何其他分类器都可以通过与贝叶斯分类器对比误差率从而评价其理想程度。</p>
</li>
</ul>
<pre><code>- 贝叶斯分类器形式如下：
  $$
  C^{Bayes}(x) = \mathop{\arg\max}_{r \in \{1, 2, \dots, K\}}
  $$

  即给定输入 $x$，贝叶斯分类器会计算 $Y$ 属于第 $r$ 类的条件概率 $P(Y = r | X = x)$，然后选择使这个条件概率最大的类。

- 设想一个二项分类的简单情形，$Y$ 有两种可能的类（下图中橘色和蓝色的点）。虚线代表了贝叶斯分类器给出的分界线，也叫做贝叶斯决策边界「Bayes Decision Boundary」。在线上，Y属于任何一类的条件概率都是 $0.5$，输入点落到任何一侧都会使成为该类的条件概率超过 $0.5$。
  ![BayesBoundary](http://pappwqm07.bkt.clouddn.com/Bayes_decision_boundary)

- 但是贝叶斯分类器的计算往往不可能实现。它代表着最理想的情况，因为计算 $P(Y = r | X = x)$ 需要我们知道数据集生成的系统性规则。而这个规则恰恰是我们想从数据中学习到的信息，是不可得的。我们只能从有限的数据中去估计、逼近这种规则。这也就是其他所有分类器以及统计方法要做的事情。
</code></pre><hr>
<ul>
<li><p><strong>线性判别分析</strong> 「LDA」</p>
<ul>
<li>先回忆一下逻辑回归「Logistic Regression」的形式<br>$$<br>P(Y = 1 | x;\theta) = h_{\theta}(x) = \frac{e^{\theta^{T}x}}{1 + e^{\theta^{T}x}}<br>$$<br>它估计的是给定输入 $X$ 的情况下，$Y$ 的<strong>条件分布</strong> 「conditional distribution」（本质上是$Y$的后验分布）。方法是通过Sigmoid函数转换线性函数，使其满足概率的基本特征。</li>
<li><p>现在我们尝试用另一个思路去给这个条件分布建模，即通过<strong>贝叶斯理论</strong>来估计它。</p>
<ol>
<li>首先假设我们知道 $Y$ 的先验分布「prior」<br>$$<br>\pi_{k} = P(Y = k)<br>$$<br>也就是任意一个样本属于第 $k$ 类的概率。</li>
<li>其次我们假设 $X$ 在给定 $Y$ 之后的条件概率<br>$$<br>f_{k}(x) \equiv P(X = x | Y = k)<br>$$</li>
<li><p>现在我们已经可以根据贝叶斯概率公式计算我们想要的条件概率了<br>$$<br>P(Y = k | X = x) = \frac{P(Y = k, X = x)}{P(X = x)}<br>$$</p>
<p>$$</p>
<pre><code>= \frac{\pi_{k}f_{k}(x)}{\sum_{l=1}^K \pi_{l}f_{l}(x)}
</code></pre><p>$$</p>
</li>
</ol>
</li>
<li><p>很明显根据上式计算出的条件分布就是一个贝叶斯分类器，代表理想情况。但我们不能精确地计算上式，因为 $\pi_{k}$ 和 $f_{k}(x)$ 都是未知的，只能通过训练数据集估计。</p>
<ol>
<li>估计 $\pi_{k}$ 很简单，计算训练集中属于 $k$ 类的样本占总样本的比例即可；<br>$$<br>\tilde{\pi_{k}} = N_{k}/N<br>$$</li>
<li>$f_{k}(x)$ 的估计则困难得多，通常我们必须对 $x$ 的分布作一些假设（如高斯分布）。不同的分布假设衍生出不同的分类方法，包括LDA、QDA、Naive Bayes等。</li>
</ol>
</li>
<li><p>LDA 有两条重要假设：</p>
<ul>
<li>$f_{k}(x)$ 符合高斯/正态分布「Guassian Distribution」<br>$$<br>f_{k}(x) = \frac{1}{(2\pi)^{p/2}|\Sigma_{k}|^{1/2}}e^{-\frac12 (x - \mu_{k})^{T}\Sigma_{k}^{-1}(x-\mu_{k})}<br>$$</li>
<li>对所有类 ${1, 2, \dots, K}$， 协方差矩阵是相同的<br>$$<br>\Sigma_{k} = \Sigma<br>$$</li>
</ul>
<p>把高斯分布代入贝叶斯公式，再取对数<br>$$<br>P(Y = k | X = x)) = \frac{\pi_{k}\frac{1}{(2\pi)^{p/2}|\Sigma_{k}|^{1/2}}e^{-\frac12 (x - \mu_{k})^{T}\Sigma_{k}^{-1}(x-\mu_{k})}}{\sum_{l=1}^K \pi_{l} \frac{1}{(2\pi)^{p/2}|\Sigma_{l}|^{1/2}}e^{-\frac12 (x - \mu_{l})^{T}\Sigma_{l}^{-1}(x-\mu_{l})}}<br>$$</p>
<p>再利用第二条假设—同协方差矩阵—整理后可得：</p>
<p>$$<br>\arg\max_{k} \log(P(Y = k | X = x)))<br>$$</p>
<p>等价于</p>
<p>$$<br>\arg\max_{k} \delta_{k}(x)<br>$$</p>
<p>$$<br>\delta_{k}(x) = x^{T}\Sigma^{-1}\mu_{k} - \frac12\mu_{k}^{T}\Sigma^{-1}\mu_{k} + \log\pi_{k}<br>$$</p>
<p>$\delta_{k}(x)$ 被称为<strong>线性判别方程</strong>。</p>
</li>
<li><p>线性判别方程对应的是贝叶斯分类器的目标函数，对其最大化就得到了贝叶斯分类器。但前面已经说过我们不知道高斯分布的具体参数，也就是我们不知道数据生成的规则。因此要用估计值来代替：</p>
<ul>
<li>$\tilde{\pi_{k}} = N_{k}/N$</li>
<li>$\tilde{\mu_{k}} = \sum_{Y_{i} = k}x_{i} / N_{k}$</li>
<li>$\tilde{\Sigma} = \sum_{k=1}^{K}\sum_{g_{i} = k}(x_{i}-\tilde{\mu_{k}})(x_{i} - \tilde{\mu_{k}})^{T} / (N - K)$</li>
</ul>
<p>代替后我们完成最优化，即是LDA。</p>
<blockquote>
<p>注：LDA与逻辑回归类似，都是构建了一个线性的decision boundary。我们计算一下LDA下的对数几率「logit」就可以看出</p>
<pre><code>$$
\log \frac{P(Y = k | X = x)}{P(Y = l | X = x)}
= \log \frac{f_{k}(x)}{f_{l}(x)} + \log \frac{\pi_{k}}{\pi_{l}}
$$
$$
=  \log \frac{\pi_{k}}{\pi_{l}} - \frac12(\mu_{k} + \mu_{l})^{T}\Sigma^{-1}(\mu_{k} - \mu_{l}) + x^{T}\Sigma^{-1}(\mu_{k} - \mu_{l})
$$
把$\pi_{k}$等参数用估计值代替后，依然能看出这是一个关于$x$的线性函数。
</code></pre></blockquote>
</li>
</ul>
</li>
</ul>
<pre><code>下图中我们用模拟生成的数据分别绘制了贝叶斯分类器和LDA给出的边界。一共模拟了三类，每一类中的数据都由高斯分布生成，并且三个分布共享同一个协方差矩阵。虚线给出了贝叶斯分类器的决策边界，右图的黑色实线代表LDA的决策边界。可以看到二者相差无几。
![LDA](http://pappwqm07.bkt.clouddn.com/LDA_boundary)

&gt; 注：由于LDA会在K纬空间中选择超平面来划分空间，它也把K维数据投影到了K-1维空间中，因此也具有降维的作用，具体请参考[这篇文章](http://www.dataivy.cn/blog/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90linear-discriminant-analysis_lda/)
</code></pre><hr>
<ul>
<li><p><strong>二次判别分析</strong> 「QDA」</p>
<p>二次判别分析「Quadratic Discriminant Analysis」也很简单，它在LDA的基础上抛弃了不同类别的高斯分布具有相同协方差矩阵的假设，也就是<br>$$<br>\Sigma_{k} \neq \Sigma_{l} \quad ~ \text{for} ~ ~ k \neq l<br>$$</p>
<p>相应地它在空间内划分出的平面不再是线性。</p>
<div align="center"><br>  <img src="http://pappwqm07.bkt.clouddn.com/QDA-boundary" width="300" height="300" alt="QDA" align="center"><br></div>
</li>
</ul>

</div>


      </div>
      
  	</div>

    <!-- After footer scripts -->
    <script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>

  </body>
</html>
